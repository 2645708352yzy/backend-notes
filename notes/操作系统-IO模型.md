[[操作系统]]
## IO模型

前置知识：

- 软中断：由操作系统内核发起的中断，用于处理硬中断未完成的工作
- 硬中断：由硬件设备发起的中断请求
- IO多路复用技术
  - 引入：使程序能同时处理多种独立事件(使得单线程能同时处理多个Socket)
  - 原理：只有在IO事件发生后，才将控制返回给应用程序。


### 内核角度网络包接收流程

<img src="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640.webp" alt="图片" style="zoom:50%;" />

### 内核角度网络包发送流程

<img src="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747557746276-3.webp" alt="图片" style="zoom:50%;" />

### [阻塞,非阻塞] [同步,异步]

**网络数据包接收流程的两个阶段**

<img src="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558078618-6.webp" alt="图片" style="zoom:50%;" />

- **数据准备阶段：** 在这个阶段，网络数据包到达网卡，通过`DMA`的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程`ksoftirqd`经过内核协议栈的处理，<mark>最终将数据发送到`内核Socket`的接收缓冲区中。</mark>
- **数据拷贝阶段：** 当数据到达`内核Socket`的接收缓冲区中时，此时数据存在于`内核空间`中，需要将数据`拷贝`到`用户空间`中，才能够被应用程序读取。

#### 阻塞与非阻塞

阻塞与非阻塞的区别主要发生在第一阶段：`数据准备阶段`。

当应用程序发起`系统调用read`时，线程从用户态转为内核态，读取内核`Socket`的接收缓冲区中的网络数据。

**阻塞**

如果这时内核`Socket`的接收缓冲区没有数据，那么线程就会一直`等待`，直到`Socket`接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，`系统调用read`返回。

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558181726-9.webp)

**非阻塞**

- 在第一阶段，当`Socket`的接收缓冲区中没有数据的时候，`阻塞模式下`应用线程会一直等待。`非阻塞模式下`应用线程不会等待，`系统调用`直接返回错误标志`EWOULDBLOCK`。
- 当`Socket`的接收缓冲区中有数据的时候，`阻塞`和`非阻塞`的表现是一样的，都会进入第二阶段`等待`数据从`内核空间`拷贝到`用户空间`，然后`系统调用返回`。

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558224852-12.webp)

#### 同步与异步

`同步`与`异步`主要的区别发生在第二阶段：`数据拷贝阶段`。

前边我们提到在`数据拷贝阶段`主要是将数据从`内核空间`拷贝到`用户空间`。然后应用程序才可以读取数据。

内核`Socket`的接收缓冲区有数据到达时，进入第二阶段。

**同步**

`同步模式`在数据准备好后，是由`用户线程`的`内核态`来执行`第二阶段`。所以应用程序会在第二阶段发生`阻塞`，直到数据从`内核空间`拷贝到`用户空间`，系统调用才会返回。

Linux下的 `epoll`和Mac 下的 `kqueue`都属于`同步 IO`。

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558276623-15.webp)

**异步**

`异步模式`下是由`内核`来执行第二阶段的数据拷贝操作，当`内核`执行完第二阶段，会通知用户线程IO操作已经完成，并将数据回调给用户线程。所以在`异步模式`下 `数据准备阶段`和`数据拷贝阶段`均是由`内核`来完成，不会对应用程序造成任何阻塞。

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558311100-18.webp)

### IO模型

##### **阻塞IO模型**

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558374536-24.webp)

每个请求都需要被一个独立的线程处理。

#### 非阻塞IO（NIO）

**非阻塞读**

当用户线程发起非阻塞`read`系统调用时，用户线程从`用户态`转为`内核态`，在内核中去查看`Socket`接收缓冲区是否有数据到来。

- `Socket`接收缓冲区中`无数据`，系统调用立马返回，并带有一个 `EWOULDBLOCK` 或 `EAGAIN`错误，这个阶段用户线程`不会阻塞`，也`不会让出CPU`，而是会继续`轮训`直到`Socket`接收缓冲区中有数据为止。
- `Socket`接收缓冲区中`有数据`，用户线程在`内核态`会将`内核空间`中的数据拷贝到`用户空间`，**注意**这个数据拷贝阶段，应用程序是`阻塞的`，当数据拷贝完成，系统调用返回。

**非阻塞写**

发送缓冲区中没有足够的空间容纳全部发送数据时，`非阻塞写`的特点是`能写多少写多少`，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的`轮训`尝试将`剩下的数据`写入发送缓冲区中。
##### 非阻塞IO模型

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558579208-34.webp)

**问题：**

因为在`非阻塞IO模型`下，需要<mark>用户线程</mark>去`不断地`发起`系统调用`去<mark>轮训`Socket`接收缓冲区</mark>，这就需要用户线程不断地从`用户态`切换到`内核态`，`内核态`切换到`用户态`。

#### IO多路复用

**但是**`非阻塞IO模型`最大的问题就是需要不断的发起`系统调用`去轮询各个`Socket`中的接收缓冲区是否有数据到来，`频繁`的`系统调用`随之带来了大量的上下文切换开销。

**那么如何避免频繁的系统调用同时又可以实现我们的核心需求呢？**

这就需要操作系统的内核来支持这样的操作，我们可以把<mark>频繁的轮询操作交给操作系统内核</mark>来替我们完成，这样就避免了在`用户空间`频繁的去使用系统调用来轮询所带来的性能开销。

##### select

`select`系统调用将`轮询`的操作交给了`内核`来帮助我们完成，从而避免了在`用户空间`不断的发起轮询所带来的的系统性能开销。

![图片](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/640-1747558754770-37.webp)

- 首先用户线程在发起`select`系统调用的时候会`阻塞`在`select`系统调用上。此时，用户线程从`用户态`切换到了`内核态`完成了一次`上下文切换`
- 用户线程将需要监听的`Socket`对应的文件描述符`fd`数组通过`select`系统调用传递给内核。此时，用户线程将`用户空间`中的文件描述符`fd`数组`拷贝`到`内核空间`。

**文件描述符fd**其实就是一个`整数值`，在Linux中一切皆文件，`Socket`也是一个文件。描述进程所有信息的数据结构`task_struct`中有一个属性`struct files_struct *files`，它最终指向了一个数组，数组里存放了进程打开的所有文件列表，文件信息封装在`struct file`结构体中，这个数组存放的类型就是`struct file`结构体，`数组的下标`则是我们常说的文件描述符`fd`。

- 当用户线程调用完`select`后开始进入`阻塞状态`，`内核`开始轮询遍历<mark>`fd`数组</mark>，查看`fd`对应的`Socket`接收缓冲区中是否有数据到来。如果有数据到来，则将`fd`对应`BitMap`的值设置为`1`。如果没有数据到来，则保持值为`0`。

- 内核遍历一遍`fd`数组后，如果发现有些`fd`上有IO数据到来，则将修改后的`fd`数组返回给用户线程。此时，会将`fd`数组从`内核空间`拷贝到`用户空间`。

- 当内核将修改后的`fd`数组返回给用户线程后，用户线程解除`阻塞`，由用户线程开始遍历`fd`数组然后找出`fd`数组中值为`1`的`Socket`文件描述符。最后对这些`Socket`发起系统调用读取数据。

- 由于内核在遍历的过程中已经修改了`fd`数组，所以在用户线程遍历完`fd`数组后获取到`IO就绪`的`Socket`后，就需要`重置`fd数组，并重新调用`select`传入重置后的`fd`数组，让内核发起新的一轮遍历轮询。

**性能开销**

虽然`select`解决了`非阻塞IO模型`中频繁发起`系统调用`的问题，但是在整个`select`工作过程中，我们还是看出了`select`有些不足的地方。

- 在发起`select`系统调用以及返回时，用户线程各发生了一次`用户态`到`内核态`以及`内核态`到`用户态`的上下文切换开销。**发生2次上下文`切换`**
- 在发起`select`系统调用以及返回时，用户线程在`内核态`需要将`文件描述符集合`从用户空间`拷贝`到内核空间。以及在内核修改完`文件描述符集合`后，又要将它从内核空间`拷贝`到用户空间。**发生2次文件描述符集合的`拷贝`**。
- 虽然由原来在`用户空间`发起轮询`优化成了`在`内核空间`发起轮询但`select`不会告诉用户线程到底是哪些`Socket`上发生了`IO就绪`事件，只是对`IO就绪`的`Socket`作了标记，用户线程依然要`遍历`文件描述符集合去查找具体`IO就绪`的`Socket`。时间复杂度依然为`O(n)`。

#### poll

`poll`相当于是改进版的`select`，但是工作原理基本和`select`没有本质的区别。

```
int poll(struct pollfd *fds, unsigned int nfds, int timeout)
struct pollfd {
    int   fd;         /* 文件描述符 */
    short events;     /* 需要监听的事件 */
    short revents;    /* 实际发生的事件 由内核修改设置 */
};
```

`select`中使用的文件描述符集合是采用的<mark>固定长度</mark>为1024的`BitMap`结构的`fd_set`，而`poll`换成了一个`pollfd`结构<mark>没有固定长度的数组</mark>，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）

`poll`只是改进了`select`只能监听`1024`个文件描述符的数量限制，但是并没有在性能方面做出改进。和`select`上本质并没有多大差别。

- 同样需要在`内核空间`和`用户空间`中对文件描述符集合进行`轮询`，查找出`IO就绪`的`Socket`的时间复杂度依然为`O(n)`。
- 同样需要将`包含大量文件描述符的集合`整体在`用户空间`和`内核空间`之间`来回复制`，**无论这些文件描述符是否就绪**。他们的开销都会随着文件描述符数量的增加而线性增大。
- `select，poll`在每次新增，删除需要监听的socket时，都需要将整个新的`socket`集合全量传至`内核`。

`poll`同样不适用高并发的场景。依然无法解决`C10K`问题。

#### **epoll**

![image-20250519111931693](%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E6%A8%A1%E5%9E%8B.assets/image-20250519111931693.png)

