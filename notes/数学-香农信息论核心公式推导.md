[[数学]]

# 香农信息熵公式的概率论推导

本文旨在通过基本的概率论逻辑，推导香农信息论的核心公式。我们从一个基本前提出发：**如果要描述一个事件，本质上是在描述其概率分布**。

我们定义“**信息量**”为：  

> 得知一个事件的概率分布后所获得的“新知”，或者说是为了记录该事件状态所需的编码长度。

以下是推导的三个步骤。

------

## 1. 信息量与概率大小的单调反比关系

首先考虑单个事件发生所带来的信息量。

**直观理解**：  

- 一个事件发生的概率越低，当它真正发生时，我们获得的信息量就越大；  
- 反之，如果一个事件发生的概率极高，它发生的消息则几乎不提供新的信息。

**举例说明**：

- 假设“明天一条狗将担任美国总统”这一事件发生了。
  由于该事件发生的先验概率 $ p(x) $ 极低（趋近于 0），一旦发生，其带来的“惊诧度”极高，具有巨大的信息价值，足以让人记忆终生。所以如果发生了，你会说“卧槽”。老了之后也许还会向子女回忆，说永远忘不了 2025 年的那天，一条狗当了美国总统。
- 相反，假设“明天特朗普继续担任美国总统”。
  这是一个大概率事件（$ p(x) $ 很大），得知这一消息不会改变人们原本的认知，因此它包含的信息量极小。所以如果我和你说这个，你会说是废话，因为它极大概率发生，没什么信息量。

**由此可得**，事件 $ x $ 的信息量 $ I(x) $ 应该是其发生概率 $ p(x) $ 的函数，且满足以下性质：

- $ I(p(x)) $ 是关于 $ p(x) $ 的**单调递减函数**；
- $ I(p(x)) \geq 0 $（信息量非负）。

------

## 2. 独立事件的可加性与对数函数

接下来考虑两个**独立事件** $ A $ 和 $ B $。

- 如果事件 $ A $ 与事件 $ B $ 相互独立，意味着得知事件 $ A $ 的结果不会改变对事件 $ B $ 的认知。
- 此时，如果我们需要同时描述这两个事件（即事件 $ A $ 和 $ B $ 同时发生），所获得的**总信息量应当等于各自信息量之和**。

数学上，独立事件的联合概率为： $$ p(A,B) = p(A) \cdot p(B) $$

根据上述**可加性假设**，我们需要寻找一个函数 $ f $，使得： $$ I(p(A,B)) = I(p(A)) + I(p(B)) $$ 即： $$ f(p(A) \cdot p(B)) = f(p(A)) + f(p(B)) $$

在数学中，能够将乘法转化为加法的**连续函数是对数函数**。因此，信息量的函数形式必定包含对数项： $$ I(x) = \log(p(x)) $$

为了与计算机科学中的二进制（比特）相兼容，我们通常取以 2 为底的对数。

------

## 3. 系统的平均信息量（熵）与符号修正

当我们不再关注单个具体的发生结果，而是关注整个系统（包含多种可能情况）时，需要计算该系统带来的**平均信息量**。

假设一个系统（随机变量）有多种可能的状态，每种状态发生的概率不尽相同。那么，该系统的总信息量应当是所有可能状态信息量的**加权平均值**（数学期望），权重即为该状态发生的概率 $ p(x) $。

根据期望公式： $$ H = \sum \left[ p(x) \cdot I(x) \right] $$

代入第二步得到的 $ I(x) = \log(p(x)) $，得到： $$ H = \sum \left[ p(x) \cdot \log(p(x)) \right] $$

**问题出现了**：
由于概率 $ p(x) \in [0, 1] $，其对数 $ \log(p(x)) $ 始终为**负值**（或零）。这会导致计算出的信息量为负数，且 $ \log(p) $ 是单调递增的（$ p $ 越大，$ \log(p) $ 越接近 0，负得越少），这与第一步中“信息量应随概率增加而减少”且“信息量应为正”的要求不符。

**解决方案**：
在公式前加上**负号**。这不仅将结果变为正值，也使得单项信息量 $ -\log(p(x)) $ 变成了关于 $ p(x) $ 的**单调递减函数**（$ p $ 越小，$ -\log(p) $ 越大），完全符合步骤 1 的设定。

------

## 最终公式：香农信息熵

综上，我们得到了**香农信息熵**的定义公式：

$$ H(X) = - \sum p(x) \log_2 p(x) $$

这一公式完全基于对**概率、独立性以及数学期望**的自然推导，构成了现代信息论的基石。